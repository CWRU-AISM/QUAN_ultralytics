{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd09f83",
   "metadata": {},
   "source": [
    "# Markov Chain Monte Carlo (MCMC) Methods: Mathematical Foundations\n",
    "\n",
    "MCMC methods represent a class of algorithms for sampling from probability distributions where direct sampling is difficult or impossible. These methods construct a Markov chain whose stationary distribution matches the target distribution of interest.\n",
    "\n",
    "## Core Mathematical Concepts\n",
    "\n",
    "### 1. Target Distribution\n",
    "\n",
    "We typically want to sample from a probability distribution with density π(x), where:\n",
    "- π(x) is known up to a normalizing constant\n",
    "- π(x) = f(x)/Z where f(x) can be calculated, but Z = ∫f(x)dx is intractable\n",
    "\n",
    "### 2. Markov Chains\n",
    "\n",
    "A Markov chain is a sequence of random variables X₀, X₁, X₂, ... such that the conditional distribution of Xₙ₊₁ given X₀, ..., Xₙ depends only on Xₙ:\n",
    "\n",
    "$$P(X_{n+1} \\in A | X_0, X_1, ..., X_n) = P(X_{n+1} \\in A | X_n)$$\n",
    "\n",
    "The transition kernel P(x, y) gives the probability density of moving from state x to state y.\n",
    "\n",
    "### 3. Stationary Distribution\n",
    "\n",
    "A distribution π is stationary for a Markov chain if:\n",
    "\n",
    "$$\\pi(y) = \\int \\pi(x)P(x, y)dx$$\n",
    "\n",
    "This means that if Xₙ ~ π, then Xₙ₊₁ ~ π as well.\n",
    "\n",
    "### 4. Detailed Balance\n",
    "\n",
    "A sufficient (but not necessary) condition for π to be the stationary distribution is detailed balance:\n",
    "\n",
    "$$\\pi(x)P(x, y) = \\pi(y)P(y, x)$$\n",
    "\n",
    "Most MCMC methods are designed to satisfy this condition.\n",
    "\n",
    "### 5. Ergodicity\n",
    "\n",
    "A Markov chain is ergodic if it is:\n",
    "- Irreducible: Can get from any state to any other state with positive probability\n",
    "- Aperiodic: The chain doesn't get trapped in cycles\n",
    "- Positive recurrent: Expected return time to any state is finite\n",
    "\n",
    "For an ergodic chain, the stationary distribution is unique, and the chain will converge to it regardless of the starting point.\n",
    "\n",
    "## Major MCMC Methods\n",
    "\n",
    "### 1. Metropolis-Hastings Algorithm\n",
    "\n",
    "The most fundamental MCMC method with the following steps:\n",
    "\n",
    "1. Start with an initial state X₀\n",
    "2. For t = 0, 1, 2, ...:\n",
    "   - Sample a proposal Y ~ q(·|Xₜ) from a proposal distribution q\n",
    "   - Calculate the acceptance ratio:\n",
    "     $$\\alpha = \\min\\left(1, \\frac{\\pi(Y)q(X_t|Y)}{\\pi(X_t)q(Y|X_t)}\\right)$$\n",
    "   - With probability α, set Xₜ₊₁ = Y; otherwise, set Xₜ₊₁ = Xₜ\n",
    "\n",
    "**Mathematical properties:**\n",
    "- The transition kernel is: P(x, y) = q(y|x)α(x, y) for x ≠ y\n",
    "- The detailed balance condition is satisfied: π(x)P(x, y) = π(y)P(y, x)\n",
    "- The algorithm works even when π is only known up to a normalizing constant\n",
    "\n",
    "**Special cases:**\n",
    "- When q is symmetric (q(y|x) = q(x|y)), the acceptance ratio simplifies to min(1, π(Y)/π(Xₜ))\n",
    "- The original Metropolis algorithm uses a symmetric proposal\n",
    "\n",
    "### 2. Gibbs Sampling\n",
    "\n",
    "A special case of Metropolis-Hastings where proposals are always accepted (α = 1).\n",
    "\n",
    "For a d-dimensional target distribution with x = (x₁, ..., xd):\n",
    "\n",
    "1. Start with an initial state X₀\n",
    "2. For t = 0, 1, 2, ...:\n",
    "   - For i = 1, ..., d:\n",
    "     - Sample X^(t+1)ᵢ ~ π(·|X^(t+1)₁, ..., X^(t+1)ᵢ₋₁, X^(t)ᵢ₊₁, ..., X^(t)d)\n",
    "     - (i.e., sample from the conditional distribution of xᵢ given all other variables)\n",
    "\n",
    "**Mathematical properties:**\n",
    "- This algorithm samples one component at a time, conditioning on all others\n",
    "- Proposal distribution q(y|x) places probability 1 on states that differ from x in exactly one component\n",
    "- Detailed balance is automatically satisfied\n",
    "- Works well when conditional distributions are easy to sample from\n",
    "\n",
    "### 3. Slice Sampling\n",
    "\n",
    "A technique that introduces an auxiliary variable to sample from π(x).\n",
    "\n",
    "1. Start with an initial state X₀\n",
    "2. For t = 0, 1, 2, ...:\n",
    "   - Sample u ~ Uniform(0, π(Xₜ))\n",
    "   - Define the \"slice\" S = {x : π(x) ≥ u}\n",
    "   - Sample Xₜ₊₁ uniformly from S\n",
    "\n",
    "**Mathematical properties:**\n",
    "- Transforms the problem into uniform sampling from a (potentially complicated) region\n",
    "- Joint distribution of (X,u) is uniform over {(x,u) : 0 ≤ u ≤ π(x)}\n",
    "- Marginal distribution of X is proportional to π(x)\n",
    "- Detailed balance is satisfied for the joint chain\n",
    "\n",
    "### 4. Hamiltonian Monte Carlo (HMC)\n",
    "\n",
    "Uses Hamiltonian dynamics to propose distant states with high acceptance probability.\n",
    "\n",
    "1. Introduce momentum variables p and define Hamiltonian H(x,p) = U(x) + K(p)\n",
    "   - U(x) = -log π(x) (potential energy)\n",
    "   - K(p) = pᵀp/2 (kinetic energy, assuming mass matrix = I)\n",
    "2. For each iteration:\n",
    "   - Sample momentum p ~ N(0,I)\n",
    "   - Simulate Hamiltonian dynamics for L steps using the leapfrog integrator:\n",
    "     ```\n",
    "     p' = p - (ε/2)∇U(x)\n",
    "     x' = x + εp'\n",
    "     p' = p' - (ε/2)∇U(x')\n",
    "     ```\n",
    "   - Accept or reject (x',p') with Metropolis acceptance probability:\n",
    "     $$\\alpha = \\min\\left(1, \\exp(H(x,p) - H(x',p'))\\right)$$\n",
    "\n",
    "**Mathematical properties:**\n",
    "- The leapfrog integrator is symplectic (preserves volume in phase space)\n",
    "- Exact Hamiltonian dynamics would preserve H(x,p), giving α = 1\n",
    "- Numerical integration introduces small errors, requiring the Metropolis correction\n",
    "- HMC can make large moves in the state space while maintaining high acceptance rates\n",
    "- Requires gradient information ∇U(x) = -∇log π(x)\n",
    "\n",
    "### 5. No-U-Turn Sampler (NUTS)\n",
    "\n",
    "An extension of HMC that automatically tunes the path length parameter L.\n",
    "\n",
    "1. Build a binary tree of states by integrating both forward and backward in time\n",
    "2. Double the tree size until a \"U-turn\" is detected (when further integration starts decreasing the distance between positions)\n",
    "3. Sample from the set of points in the tree with appropriate probabilities to maintain detailed balance\n",
    "\n",
    "**Mathematical properties:**\n",
    "- Eliminates the need to hand-tune the number of leapfrog steps L\n",
    "- Adapts the trajectory length to the local geometric properties of the target distribution\n",
    "- Maintains detailed balance despite the adaptive procedure\n",
    "- Particularly effective for high-dimensional distributions with varying scales\n",
    "\n",
    "### 6. Metropolis-Adjusted Langevin Algorithm (MALA)\n",
    "\n",
    "Combines Langevin dynamics with a Metropolis acceptance step.\n",
    "\n",
    "1. Propose moves using a discretized Langevin diffusion:\n",
    "   $$Y = X_t + \\frac{\\epsilon}{2}\\nabla \\log \\pi(X_t) + \\sqrt{\\epsilon}Z$$\n",
    "   where Z ~ N(0,I)\n",
    "2. Accept or reject using Metropolis-Hastings\n",
    "\n",
    "**Mathematical properties:**\n",
    "- Uses gradient information to guide proposals toward higher-probability regions\n",
    "- The proposal distribution is:\n",
    "  $$q(y|x) = N\\left(y; x + \\frac{\\epsilon}{2}\\nabla \\log \\pi(x), \\epsilon I\\right)$$\n",
    "- Less computationally intensive than full HMC but still leverages gradient information\n",
    "- Efficiency depends critically on the step size ε\n",
    "\n",
    "## Convergence Diagnostics\n",
    "\n",
    "### 1. Effective Sample Size (ESS)\n",
    "\n",
    "Measures the equivalent number of independent samples represented by the MCMC chain:\n",
    "\n",
    "$$\\text{ESS} = \\frac{n}{1 + 2\\sum_{k=1}^{\\infty}\\rho_k}$$\n",
    "\n",
    "where n is the chain length and ρₖ is the autocorrelation at lag k.\n",
    "\n",
    "### 2. Potential Scale Reduction Factor (PSRF or R̂)\n",
    "\n",
    "Compares the variance within each chain to the variance between chains:\n",
    "\n",
    "$$\\hat{R} = \\sqrt{\\frac{W + B/m}{W}}$$\n",
    "\n",
    "where W is the within-chain variance, B is the between-chain variance, and m is the chain length.\n",
    "\n",
    "Values of R̂ close to 1 indicate convergence.\n",
    "\n",
    "## Practical Considerations\n",
    "\n",
    "### 1. Burn-in Period\n",
    "\n",
    "Initial samples are influenced by the starting point and are typically discarded:\n",
    "\n",
    "$$\\{X_{b+1}, X_{b+2}, ..., X_n\\}$$\n",
    "\n",
    "where b is the burn-in period.\n",
    "\n",
    "### 2. Thinning\n",
    "\n",
    "To reduce autocorrelation, we can keep every kth sample:\n",
    "\n",
    "$$\\{X_k, X_{2k}, ..., X_{\\lfloor n/k \\rfloor k}\\}$$\n",
    "\n",
    "### 3. Adaptation\n",
    "\n",
    "Many modern MCMC methods use adaptive schemes to tune parameters during the burn-in phase:\n",
    "- Adaptive Metropolis adjusts the proposal covariance\n",
    "- Adaptive HMC tunes both step size ε and mass matrix\n",
    "\n",
    "## Theoretical Convergence Results\n",
    "\n",
    "The fundamental convergence theorem for MCMC states that for an ergodic Markov chain with stationary distribution π:\n",
    "\n",
    "$$\\lim_{n \\to \\infty} \\frac{1}{n}\\sum_{i=1}^{n}f(X_i) \\stackrel{\\text{a.s.}}{=} \\int f(x)\\pi(x)dx$$\n",
    "\n",
    "for any integrable function f, meaning sample averages converge to expectations under the target distribution.\n",
    "\n",
    "The convergence rate is typically O(n^(-1/2)), the same as Monte Carlo integration with independent samples, but with a constant that depends on the autocorrelation of the chain.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebc929b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
